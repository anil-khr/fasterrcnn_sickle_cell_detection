{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anil\\Downloads\\faster_rcnn_od\\fasterrcnn_sickle_cell_detection\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\anil\\Downloads\\faster_rcnn_od\\fasterrcnn_sickle_cell_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "from tqdm.auto import tqdm\n",
    "from faster_rcnn_od.utils.comman import custom_collate, Averager, SaveBestModel, save_loss_plot, save_model, CustDat\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = namedtuple(\"train_faster_rcnn\", [\n",
    "'root_dir','model_loader_path',\n",
    " 'train_loader_path','valid_loader_path',\n",
    " 'outputs', 'params_epoches', 'params_lr',\n",
    " 'params_momentum', 'params_weight_decay', 'params_batch_size'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_rcnn_od.constants import *\n",
    "from faster_rcnn_od.utils.comman import read_yaml, create_directories, custom_collate, CustDat\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def train_config(self) -> trainConfig:\n",
    "        config = self.config.train_faster_rcnn\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        train_config = trainConfig (\n",
    "            root_dir=config.root_dir,\n",
    "            model_loader_path=config.model_loader_path,\n",
    "            train_loader_path=config.train_loader_path,\n",
    "            valid_loader_path= config.valid_loader_path,\n",
    "            outputs= config.outputs,\n",
    "            params_epoches=self.params.EPOCHS,\n",
    "            params_lr=self.params.LEARNING_RATE,\n",
    "            params_momentum=self.params.MOMENTUM,\n",
    "            params_weight_decay = self.params.WEIGHT_DECAY,\n",
    "            params_batch_size = self.params.BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        return train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training:\n",
    "    \n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    def __init__(self, config: trainConfig):\n",
    "        \n",
    "        self.config = config\n",
    "        print(self.config)\n",
    "        \n",
    "    def base_model(self):\n",
    "        return torch.load(self.config.model_loader_path)\n",
    "    \n",
    "    def train_start(self):\n",
    "        \n",
    "        # print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "        # print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "        # initialize the model and move to the computation device\n",
    "        model = self.base_model()\n",
    "        model = model.to(self.DEVICE)\n",
    "        # get the model parameters\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.SGD(params, lr=self.config.params_lr, momentum=self.config.params_momentum, weight_decay=self.config.params_weight_decay)\n",
    "        # initialize the Averager class\n",
    "        train_loss_hist = Averager()\n",
    "        val_loss_hist = Averager()\n",
    "        self.train_itr = 1\n",
    "        self.val_itr = 1\n",
    "        # train and validation loss lists to store loss values of all...\n",
    "        # ... iterations till ena and plot graphs for all iterations\n",
    "        # name to save the trained model with\n",
    "        MODEL_NAME = 'model'\n",
    "        # whether to show transformed images from data loader or not\n",
    "        save_best_model = SaveBestModel()\n",
    "        # start the training epochs\n",
    "        NUM_EPOCHS = 1\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "            # reset the training and validation loss histories for the current epoch\n",
    "            train_loss_hist.reset()\n",
    "            val_loss_hist.reset()\n",
    "            # start timer and carry out training and validation\n",
    "            start = time.time()\n",
    "            train_loss, loss_train = self.train(self.train_data_loader('train_loder.pt'), model, optimizer)\n",
    "            val_loss, loss_val = self.validate(self.train_data_loader('val_loader.pt'), model)\n",
    "            train_loss_hist.send(loss_train)\n",
    "            train_loss_hist.send(loss_val)\n",
    "            print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "            print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "            end = time.time()\n",
    "            print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "            # save the best model till now if we have the least loss in the...\n",
    "            # ... current epoch\n",
    "            save_best_model(\n",
    "                val_loss_hist.value, epoch, model, optimizer\n",
    "            )\n",
    "            # save the current epoch model\n",
    "            save_model(epoch, model, optimizer)\n",
    "            # save loss plot\n",
    "            OUT_DIR =self.config.outputs\n",
    "            save_loss_plot(OUT_DIR, train_loss, val_loss)\n",
    "            \n",
    "            # sleep for 5 seconds after each epoch\n",
    "            time.sleep(5)\n",
    "        \n",
    "    def train_data_loader(self, file_name):  \n",
    "        path = self.config.train_loader_path\n",
    "        path_tr = os.path.join(path, file_name)\n",
    "        data = torch.load(path_tr)\n",
    "        for i in data:\n",
    "            imgs = []\n",
    "            targets = []\n",
    "            for d in i:\n",
    "                imgs.append(d[0])\n",
    "                targ = {}\n",
    "                targ['boxes'] = d[1]['boxes']#.to(device)\n",
    "                targ['labels'] = d[1]['label']#.to(device)\n",
    "                targets.append(targ)\n",
    "        return (targets, imgs)\n",
    "        \n",
    "    def train(self,train_data_loader, model, optimizer):\n",
    "        \n",
    "        print('Training')\n",
    "        global train_itr\n",
    "        global train_loss_list\n",
    "        train_loss_list = []\n",
    "        \n",
    "        # initialize tqdm progress bar\n",
    "        prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "        targets, images = train_data_loader\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict[0].values())\n",
    "            loss_value = losses.item()\n",
    "            train_loss_list.append(loss_value)\n",
    "            train_loss_hist = Averager()\n",
    "            train_loss_hist.send(loss_value)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            self.train_itr += 1\n",
    "            \n",
    "        \n",
    "            # update the loss value beside the progress bar for each iteration\n",
    "            prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "        return train_loss_list, loss_value\n",
    "    \n",
    "    \n",
    "    def validate(self, valid_data_loader, model ):\n",
    "        print('Validating')\n",
    "        global val_itr\n",
    "        global val_loss_list\n",
    "        val_loss_list = []\n",
    "        \n",
    "        # initialize tqdm progress bar\n",
    "        prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "        \n",
    "        for i, data in enumerate(prog_bar):\n",
    "            targets, images  = valid_data_loader\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict[0].values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_list.append(loss_value)\n",
    "            \n",
    "           # val_loss_hist.send(loss_value)\n",
    "            self.val_itr += 1\n",
    "            # update the loss value beside the progress bar for each iteration\n",
    "            prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "        return val_loss_list, loss_value\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-07 18:58:55,749: INFO: comman: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-05-07 18:58:55,754: INFO: comman: yaml file: params.yaml loaded successfully]\n",
      "[2023-05-07 18:58:55,755: INFO: comman: created directory at: artifacts]\n",
      "[2023-05-07 18:58:55,757: INFO: comman: created directory at: artifacts/trained_output_facts]\n",
      "train_faster_rcnn(root_dir='artifacts/trained_output_facts', model_loader_path='artifacts/prepare_base_model/base_model.pt', train_loader_path='artifacts\\\\transformed_data', valid_loader_path='artifacts\\\\transformed_data\\\\val_loader.pt', outputs='artifacts/trained_output_facts', params_epoches=1, params_lr=0.01, params_momentum=0.9, params_weight_decay=0.0005, params_batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anil\\anaconda3\\envs\\fasterrcnn\\lib\\site-packages\\torch\\serialization.py:799: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]code/__torch__/torchvision/models/detection/faster_rcnn.py:103: UserWarning: RCNN always returns a (Losses, Detections) tuple in scripting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'loss_classifier': tensor(1.9353, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.8701, grad_fn=<DivBackward1>), 'loss_objectness': tensor(2.5696, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.3510, grad_fn=<DivBackward1>)}, [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5.7260:  50%|█████     | 1/2 [00:07<00:07,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'loss_classifier': tensor(1.0300, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.8499, grad_fn=<DivBackward1>), 'loss_objectness': tensor(0.7337, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1522, grad_fn=<DivBackward1>)}, [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.7658: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6.5485: 100%|██████████| 2/2 [00:05<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 train loss: 4.657\n",
      "Epoch #1 validation loss: 0.000\n",
      "Took 0.561 minutes for epoch 0\n",
      "\n",
      "Best validation loss: 0\n",
      "\n",
      "Saving best model for epoch: 1\n",
      "\n",
      "SAVING PLOTS COMPLETE...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    train_model_config = config.train_config()\n",
    "    prepare_base_model = training(config=train_model_config)\n",
    "    prepare_base_model.train_start()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasterrcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
